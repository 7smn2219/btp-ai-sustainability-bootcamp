{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba04062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPUT_PATH=/Users/I559573/Downloads/D2V2.0/D2V_Datasets/ImageSamples\n",
      "env: DATA_SOURCE=/Users/I559573/Downloads/D2V2.0/D2V_Datasets/ImageSamples/lgp_dataset\n"
     ]
    }
   ],
   "source": [
    "%env OUTPUT_PATH=/Users/I559573/Downloads/D2V2.0/D2V_Datasets/ImageSamples\n",
    "%env DATA_SOURCE=/Users/I559573/Downloads/D2V2.0/D2V_Datasets/ImageSamples/lgp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f53d93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 17:40:29,690:root:INFO - Loading classifier pipeline from /Users/I559573/Downloads/D2V2.0/D2V_Datasets/ImageSamples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 0.7013 - accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 17:40:44,226:root:INFO - -----START INFERENCE-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 17:40:44,752:root:INFO - The input was predicted as 'Anomalous'\n",
      "2022-02-23 17:40:44,754:root:INFO - -----END INFERENCE-----\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Training script to showcase the end-to-end training and evaluation script.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "import cv2\n",
    "import joblib\n",
    "import os\n",
    "import keras\n",
    "\n",
    "#from sapai import tracking\n",
    "from os.path import exists\n",
    "from joblib import load, dump\n",
    "from os import makedirs\n",
    "from os import environ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "FORMAT = \"%(asctime)s:%(name)s:%(levelname)s - %(message)s\"\n",
    "# Use filename=\"file.log\" as a param to logging to log to a file\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "\n",
    "class TrainSKInterface:\n",
    "    def __init__(self) -> None:\n",
    "        # Set the params for the training below\n",
    "        self.image_pipeline = None\n",
    "        self.dataset_all = None\n",
    "        self.train, self.val, self.test = None, None, None\n",
    "        self.target_classes = None\n",
    "        self.dataset_name = \"lgp_dataset\"\n",
    "        self.model_name = \"classifier_pipeline.pkl\"\n",
    "        self.output_path = environ[\"OUTPUT_PATH\"]\n",
    "        self.file_name = environ[\"DATA_SOURCE\"]\n",
    "        self.loss = None\n",
    "        self.val_loss = None\n",
    "        self.accuracy = None\n",
    "        self.val_accuracy = None\n",
    "\n",
    "\n",
    "    def create_dataset_bin(self, img_folder):\n",
    "        IMG_WIDTH = 224\n",
    "        IMG_HEIGHT = 224\n",
    "        img_data_array = []\n",
    "        for file in os.listdir(img_folder):\n",
    "            image_path = os.path.join(img_folder, file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "            image = np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255\n",
    "            image = image.tobytes()\n",
    "            img_data_array.append(image)\n",
    "        return img_data_array \n",
    "    \n",
    "\n",
    "    def read_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reads the images file from path\n",
    "        \"\"\"\n",
    "        \n",
    "        path_img_ok = self.file_name + \"/Images/OK/\"\n",
    "        path_img_ko = self.file_name + \"/Images/NG/\"\n",
    "        \n",
    "        #logging.info(f\"{path_img_ok}\")\n",
    "        #logging.info(f\"{path_img_ko}\")\n",
    "        \n",
    "        img_dataset_ok_bin = self.create_dataset_bin(path_img_ok)\n",
    "        img_dataset_ko_bin = self.create_dataset_bin(path_img_ko)\n",
    "\n",
    "        df_img_dataset_ok = pd.DataFrame(columns = ['image','label'])\n",
    "        df_img_dataset_ok['image'] = img_dataset_ok_bin\n",
    "        df_img_dataset_ok['label'] = 0\n",
    "        df_img_dataset_ko = pd.DataFrame(columns = ['image','label'])\n",
    "        df_img_dataset_ko['image'] = img_dataset_ko_bin\n",
    "        df_img_dataset_ko['label'] = 1\n",
    "\n",
    "        self.dataset_all = pd.concat([df_img_dataset_ok,df_img_dataset_ko], ignore_index=True)\n",
    "        self.dataset_all = self.dataset_all.sample(frac=1).reset_index(drop=True)\n",
    "        self.target_classes = self.dataset_all[\"label\"].unique()\n",
    "        #print(f\"No. of training examples: {self.dataset_all.shape[0]}\")\n",
    "        #print(f\"Classes: {self.target_classes}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "    def split_dataset(self) -> None:\n",
    "        \"\"\"\n",
    "        Split the dataset into train, validate and test\n",
    "\n",
    "        Raises:\n",
    "            Error: if dataset_train and dataset_test are not set\n",
    "        \"\"\"\n",
    "        if self.dataset_all is None:\n",
    "            raise Exception(\"Train or test data not set\")\n",
    "\n",
    "        #Change splitting proportions\n",
    "        self.train, self.val = train_test_split(self.dataset_all, test_size=0.97, random_state=25)\n",
    "        self.val, self.test = train_test_split(self.val, test_size=0.97, random_state=25)\n",
    "\n",
    "        #print(f\"No. of training examples: {self.train.shape[0]}\")\n",
    "        #print(f\"No. of validation examples: {self.val.shape[0]}\")\n",
    "        #print(f\"No. of test examples: {self.test.shape[0]}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def convert_back(self, df):\n",
    "        \n",
    "        temp_arr = []\n",
    "        for i in df['image'].values:\n",
    "            a = np.frombuffer(i, dtype=np.float32)\n",
    "            a = a.reshape(224,224,3)\n",
    "            temp_arr.append(a)\n",
    "            #print(a.shape)\n",
    "            \n",
    "        return temp_arr\n",
    "\n",
    "\n",
    "    def prepare_model(self):\n",
    "    \n",
    "        base_model = tf.keras.applications.vgg16.VGG16(\n",
    "            input_shape = (224, 224, 3), # Shape of our images\n",
    "            include_top = False, # Leave out the last fully connected layer\n",
    "            weights = 'imagenet'\n",
    "        )\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "            if(layer.name == 'block4_conv1'):\n",
    "                break\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Flatten the output layer to 1 dimension\n",
    "        x = layers.Flatten()(base_model.output)\n",
    "\n",
    "        # Add a fully connected layer with 512 hidden units and ReLU activation\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "        # Add a dropout rate of 0.5\n",
    "        #x = layers.Dropout(0.5)(x) #To be uncommented\n",
    "\n",
    "        # Add a final sigmoid layer with 1 node for classification output\n",
    "        x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        self.image_pipeline = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "        self.image_pipeline.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "                      loss = 'binary_crossentropy', \n",
    "                      metrics = ['accuracy']\n",
    "                     )\n",
    "        \n",
    "        #self.image_pipeline.summary()\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "    def train_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Train and save the model\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "        #config = tf.compat.v1.ConfigProto(device_count = {'GPU': 0, 'CPU': 8}) \n",
    "        #sess = tf.compat.v1.Session(config=config) \n",
    "        #keras.backend.set_session(sess)\n",
    "        \n",
    "        img_train = self.convert_back(self.train)\n",
    "        img_val = self.convert_back(self.val)\n",
    "        \n",
    "        #print(len(img_train))\n",
    "        #print(len(img_val))\n",
    "        #print(img_train[0].shape)\n",
    "        #print(img_val[0].shape)\n",
    "\n",
    "        history = self.image_pipeline.fit(\n",
    "            x=np.array(img_train, np.float32), \n",
    "            y=np.array(list(map(int,self.train['label'])), np.float32), \n",
    "            validation_data = (np.array(img_val, np.float32), self.val['label'].values)\n",
    "            #,steps_per_epoch = 100\n",
    "            ,epochs = 1 #To be changed\n",
    "        )\n",
    "        \n",
    "        self.loss = history.history['loss']\n",
    "        self.val_loss = history.history['val_loss']\n",
    "        self.accuracy = history.history['accuracy']\n",
    "        self.val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Saves the model to the local path\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Writing tokenizer into {self.output_path}\")\n",
    "        if not exists(self.output_path):\n",
    "            makedirs(self.output_path)\n",
    "        # Save the Tokenizer and target classes to pickle file\n",
    "        with open(f\"{self.output_path}/{self.model_name}\", \"wb\") as handle:\n",
    "            dump([self.image_pipeline, self.target_classes], handle)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def get_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Get the model if it is available locally\n",
    "        \"\"\"\n",
    "        \n",
    "        if exists(f\"{self.output_path}/{self.model_name}\"):\n",
    "            logging.info(f\"Loading classifier pipeline from {self.output_path}\")\n",
    "            with open(f\"{self.output_path}/{self.model_name}\", \"rb\") as handle:\n",
    "                [self.image_pipeline, self.target_classes] = load(handle)\n",
    "        else:\n",
    "            logging.info(f\"Model has not been trained yet!\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def model_metrics(self):\n",
    "        \"\"\"\n",
    "        Perform an inference on the model that was trained\n",
    "        \"\"\"\n",
    "        if self.image_pipeline is None:\n",
    "            self.get_model()\n",
    "\n",
    "        infer_data = np.array(self.convert_back(self.test), np.float32)\n",
    "        infer_data_labels = self.test['label'].values\n",
    "        \n",
    "        score = self.image_pipeline.evaluate(infer_data[0:10], infer_data_labels[0:10])\n",
    "        #print(\"Accuracy: \" + str(score[0]))\n",
    "\n",
    "        metric = [\n",
    "            {\"name\": \"Model accuracy\",\n",
    "            \"value\": float(score[1]),\n",
    "            \"labels\":[{\"name\": \"dataset\", \"value\": \"test set\"}]}\n",
    "            ]\n",
    "        #print(metric)\n",
    "        #tracking.log_metrics(metric, artifact_name = \"defect-detection\")\n",
    "        \n",
    "        training_metrics = [\n",
    "                    {'loss': str(self.loss)},\n",
    "                    {'val_loss': str(self.val_loss)},\n",
    "                    {'accuracy': str(self.accuracy)},\n",
    "                    {'val_accuracy': str(self.val_accuracy)}\n",
    "                ]\n",
    "        custom_info_1 = [{\"name\": \"Metrics\", \"value\": str(training_metrics)}]\n",
    "\n",
    "        #print(custom_info_1)\n",
    "        #tracking.set_custom_info(custom_info_1)\n",
    "        \n",
    "        #confusion matrix\n",
    "        y_pred = np.round(self.image_pipeline.predict(infer_data[0:10]), 0)\n",
    "        cnf_matrix = confusion_matrix(infer_data_labels[0:10], y_pred)\n",
    "        #print(cnf_matrix)\n",
    "        cf_matrix = [\n",
    "                        {'actual label - 0': str(cnf_matrix[0])},\n",
    "                        {'actual label - 1': str(cnf_matrix[1])}\n",
    "                    ]\n",
    "        custom_info_2 = [{\"name\": \"Confusion Matrix (columns: predicted-class, rows: actual-class)\",\n",
    "                    \"value\": str(cf_matrix)}]\n",
    "\n",
    "        #print(custom_info_2)\n",
    "        #tracking.set_custom_info(custom_info_2)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def infer_model(self) -> str:\n",
    "        \"\"\"\n",
    "        Perform an inference on the model that was trained\n",
    "        \"\"\"\n",
    "        if self.image_pipeline is None:\n",
    "            self.get_model()\n",
    "\n",
    "        infer_data = np.array(self.convert_back(self.test), np.float32)\n",
    "        \n",
    "        img_1 = infer_data[0:1]\n",
    "        image_input = {\n",
    "            \"image\": img_1.tolist()\n",
    "        }\n",
    "        \n",
    "        print(img_1.shape)\n",
    "        print(type(img_1))\n",
    "        \n",
    "        logging.info(f\"-----START INFERENCE-----\")\n",
    "        prediction = self.image_pipeline.predict(np.array(image_input[\"image\"], np.float32))\n",
    "        predicted_label = \"Anomalous\" if prediction[0] > 0.5 else \"Normal\"\n",
    "        logging.info(f\"The input was predicted as '{predicted_label}'\")\n",
    "        logging.info(f\"-----END INFERENCE-----\")\n",
    "\n",
    "        return predicted_label\n",
    "\n",
    "\n",
    "    def run_workflow(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the training script with all the necessary steps\n",
    "        \"\"\"\n",
    "        self.read_dataset()\n",
    "        self.split_dataset()\n",
    "            \n",
    "        self.get_model()\n",
    "        if self.image_pipeline is None:\n",
    "            # Train the model if no model is available\n",
    "            logging.info(f\"Training classifier and saving it locally\")\n",
    "            self.prepare_model()\n",
    "            self.train_model()\n",
    "            self.save_model()\n",
    "\n",
    "        self.model_metrics()\n",
    "        self.infer_model()\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_obj = TrainSKInterface()\n",
    "    train_obj.run_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d34b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
