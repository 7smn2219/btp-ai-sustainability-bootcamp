{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fc96bb",
   "metadata": {},
   "source": [
    "# BYOM with Tensorflow: Sound Based Predictive Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your service key JSON file relative to this notebook\n",
    "aic_service_key_path = '../files/aic_service_key.json'\n",
    "\n",
    "# Loads the service key file\n",
    "with open(aic_service_key_path) as ask:\n",
    "    aic_service_key = json.load(ask)\n",
    "\n",
    "# Creating an AI API client instance\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url = aic_service_key[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\", # The present AI API version is 2\n",
    "    auth_url=  aic_service_key[\"url\"] + \"/oauth/token\",\n",
    "    client_id = aic_service_key['clientid'],\n",
    "    client_secret = aic_service_key['clientsecret']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0397b-cb14-4482-9334-ff451584258f",
   "metadata": {},
   "source": [
    "### Already done in defect detection exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f37ff5-b1c3-4c0e-9a84-aca63dee41e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create and upload the docker images\n",
    "\n",
    "Start docker on your machine and execute the following commands in a terminal:\n",
    "\n",
    "- docker login docker.io -u YOUR_DOCKER_REPO\n",
    "\n",
    "- docker build code/train -t YOUR_DOCKER_REPO/sound-train:0.0.1 .\n",
    "\n",
    "- docker push docker.io/YOUR_DOCKER_REPO/sound-train:0.0.1\n",
    "\n",
    "NB: you should be connected to SAP VPN to download sap-ai-tracking module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b9ee5",
   "metadata": {},
   "source": [
    "In order to make the API call, let's transform the array into a binary string:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0e57c",
   "metadata": {},
   "source": [
    "# Create Resource Group and Connect AWS S3 to SAP AI Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc092d19",
   "metadata": {},
   "source": [
    "Resource Groups represent a virtual collection of related resources within the scope of one SAP AI Core tenant. Previously we have created a specific resource group for our condition monitoring exercise to keep separated those resources.\n",
    "That resource group will appear also on the AI Lauchpad UI. By selecting it, all the related resource will be accessibles.\n",
    "Let's see how to add resources to the \"sound\" resource group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d7239",
   "metadata": {},
   "source": [
    "AI Core uses AWS S3 Object Store as a cloud storage for your datasets and models. \n",
    "You can get AWS S3 Bucket from either of two ways:\n",
    "\n",
    "- through SAP BTP Cockpit;\n",
    "\n",
    "- through AWS. Refer AWS User Guide to S3.\n",
    "\n",
    "A prerequisite to our exercise is to install AWS CLI and upload our dataset to AWS S3 Object Store by means of a command like the following:\n",
    "\n",
    "    aws s3 cp --recursive /local/path/to/data s3://your-bucket-id/sound/data/\n",
    "\n",
    "Once we have done this, we need to register the AWS S3 Object Store to SAP AI Core resource group. In order to to that we prepare a json file with the needed AWS S3 credentials and we use a proper API in order to register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads your service key\n",
    "s3_service_key_path = '../files/s3_service_key.json'\n",
    "\n",
    "# Loads the service key file\n",
    "with open(s3_service_key_path) as s3sk:\n",
    "    s3_service_key = json.load(s3sk)\n",
    "\n",
    "default_secret = {\n",
    "    \"name\": \"default\", # Name of the connection\n",
    "    \"type\": \"S3\",\n",
    "    \"endpoint\": s3_service_key[\"host\"],\n",
    "    \"bucket\": s3_service_key[\"bucket\"],\n",
    "    \"pathPrefix\": \"sound\",\n",
    "    \"region\": s3_service_key[\"region\"],\n",
    "    \"data\": {\n",
    "        \"AWS_ACCESS_KEY_ID\": s3_service_key[\"access_key_id\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": s3_service_key[\"secret_access_key\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the api\n",
    "ai_api_client.rest_client.post(\n",
    "    path=\"/admin/objectStoreSecrets\",\n",
    "    body = default_secret, # defined above\n",
    "    resource_group = \"sound\" #This specifies the resource group the AWS S3 bucket is related to\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa588c84",
   "metadata": {},
   "source": [
    "Let's make an API call to check if the artifact was corretly created. Object store secrets are stored under /admin/objectStoreSecrets and are linked to the resource group \"sound\", so those parameters have to be provided when creating and listing them. In case, with a similar call, secrets can be also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_api_client.rest_client.get(\n",
    "    path=\"/admin/objectStoreSecrets\",\n",
    "    resource_group = \"sound\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a063fd",
   "metadata": {},
   "source": [
    "It is always a good practice to check the synchronization of the workflows (registered as apps), that are read from the registered GitHub repository. \n",
    "Please, keep in mind that the synchronization is triggered by any change to the yaml files pushed to GitHub and that AI Core checks every 3 minutes for new files or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27739c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"aicore-sound\"\n",
    "\n",
    "ai_api_client.rest_client.get(\n",
    "    path=f\"/admin/applications/{app_name}/status\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1e473",
   "metadata": {},
   "source": [
    "# Train Execution of ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import yaml\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client\n",
    "from ai_api_client_sdk.models.artifact import Artifact\n",
    "from ai_api_client_sdk.models.status import Status\n",
    "from ai_api_client_sdk.models.target_status import TargetStatus\n",
    "from ai_api_client_sdk.models.parameter_binding import ParameterBinding\n",
    "from ai_api_client_sdk.models.input_artifact_binding import InputArtifactBinding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7539c19",
   "metadata": {},
   "source": [
    "In order to be able to train our model in AI Core, we need to define an AI API client, this time by specifying also the resource group in order to access all the related resource (i.e. artifacts and scenarios) and store the subsequent objects (like configurations, models, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf34989",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"sound\"  # Must be created before\n",
    "\n",
    "aic_service_key = '../files/aic_service_key.json' # ENSURE YOU HAVE THE FILE PLACED CORRECTLY\n",
    "with open(aic_service_key) as ask:\n",
    "    aic_s_k = json.load(ask)\n",
    "\n",
    "# NO CHANGES REQUIRED BELOW\n",
    "#\n",
    "ai_api_v2_client = AIAPIV2Client(\n",
    "    base_url=aic_s_k[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\",\n",
    "    auth_url=aic_s_k[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_s_k['clientid'],\n",
    "    client_secret=aic_s_k['clientsecret'],\n",
    "    resource_group=resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c7cd6",
   "metadata": {},
   "source": [
    "Previously we have defined some scenarios (through the provided workflow yaml files) for our resource group. We can check if they are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_resp = ai_api_v2_client.scenario.query(resource_group)\n",
    "\n",
    "print(\"Scenarios\")\n",
    "print(\"---\" * 20)\n",
    "for idx, scenario in enumerate(scenario_resp.resources):\n",
    "    print(f\"Scenario ID {idx + 1} -> [{scenario.id}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286ea1b",
   "metadata": {},
   "source": [
    "In order to be used, training data need to be registered as artifact in AI Core. This means specify some data parameters and associate them to an existing scenario. Below an example on how to perform this registration in the context of our exercise.\n",
    "\n",
    "\n",
    "Please, note that ai://default/data is the path to the data in S3 and \"default\" is specified in the secret object store registration. If you have specified also a pathPrefix, that will be already include in \"default\".\n",
    "\n",
    "\n",
    "The scenario id is assigned as the one specified into the training workflows (we read it from the local file, and it should match the one read by AICore from the GitHub repository). At the end of the registrazioni step, a unique id is assigned to the artifact. Once registered the artifact will appear in the \"Dataset\" section in AI Launchpad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a8829",
   "metadata": {},
   "source": [
    "NB: This step needs to be executed only when registering a new dataset, or something has changed in the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CHANGES REQUIRED BELOW\n",
    "#\n",
    "# Load training_workflow.yaml\n",
    "training_workflow_file = '../workflows_sound/training_workflow.yaml'\n",
    "with open(training_workflow_file) as twf:\n",
    "    training_workflow = yaml.safe_load(twf)\n",
    "#\n",
    "# Load scenario id from train_workflow.yaml\n",
    "scenario_id = training_workflow['metadata']['labels']['scenarios.ai.sap.com/id']\n",
    "#\n",
    "# Set the artifact configuration\n",
    "artifact = {\n",
    "        \"name\": \"sound-data\", # Modifiable name\n",
    "        \"kind\": Artifact.Kind.DATASET,\n",
    "        \"url\": \"ai://default/data\", #default is \n",
    "        \"description\": \"Cutting machine sound clips for defect detection\",\n",
    "        \"scenario_id\": scenario_id\n",
    "    }\n",
    "# Store the artifact response to retrieve the id for the training configuration\n",
    "artifact_resp = ai_api_v2_client.artifact.create(**artifact)\n",
    "print(f\"Artifacts registered for {scenario_id} scenario!\")\n",
    "pprint(vars(artifact_resp))\n",
    "#\n",
    "# Checks if the message contains expected string\n",
    "assert artifact_resp.message == 'Artifact acknowledged'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848075f7",
   "metadata": {},
   "source": [
    "In order to train our model, we need to create a training configuration in AICore. A configuration convey several information about the training:\n",
    "\n",
    "- the name of the configuration itself;\n",
    "- the name that will indetify the input data inside AICore (it is assigned in the workflow);\n",
    "- the name that will identify the training executable in AICore (it is assigned in the workflow);\n",
    "- the id of the related scenario\n",
    "\n",
    "At the end of the configuration step, a unique id is assigned to the configuration. Once created the configuration will appear in the \"Configuration\" section in AI Launchpad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f1aae",
   "metadata": {},
   "source": [
    "NB: Also in this case, the configuration is created one time, unless something has changed or you need to execute another training on a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_artifact_name = training_workflow['spec']['templates'][0]['inputs']['artifacts'][0]['name']\n",
    "executable_name = training_workflow['metadata']['name']\n",
    "\n",
    "artifact_binding = {\n",
    "    \"key\": input_artifact_name,\n",
    "    \"artifact_id\": artifact_resp.id\n",
    "}\n",
    "\n",
    "train_configuration = {\n",
    "    \"name\": \"sound-training-configuration\",\n",
    "    \"scenario_id\": scenario_id,\n",
    "    \"executable_id\": executable_name,\n",
    "    \"parameter_bindings\": [],\n",
    "    \"input_artifact_bindings\": [ InputArtifactBinding(**artifact_binding) ]\n",
    "}\n",
    "\n",
    "# store the configuration response to access the id to create an execution\n",
    "train_config_resp = ai_api_v2_client.configuration.create(**train_configuration)\n",
    "pprint(vars(train_config_resp))\n",
    "\n",
    "assert train_config_resp.message == 'Configuration created'\n",
    "\n",
    "print(\"Configuration created for running the training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0576ac",
   "metadata": {},
   "source": [
    "Once the configuration is created, we are ready to creare an execution that will be immediatly launched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ef445",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_resp = ai_api_v2_client.execution.create(train_config_resp.id)\n",
    "pprint(vars(execution_resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa158f",
   "metadata": {},
   "source": [
    "The following code provide information about the training status. The training status can be also checked in AI Launchpad under the \"Execution\" sections.\n",
    "\n",
    "Please, note that the execution is performed inside the docker image previosly prepared and loaded in the Docker repository. The information needed to access the Docker training image are specified inside the training workflow file loaded in GitHub which AICore is synchronized with.\n",
    "\n",
    "Once the training is complete, a model is saved and it becomes available under the AI Launchpad \"Models\" section for deploying a serving artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35210f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = None\n",
    "while status != Status.COMPLETED and status != Status.DEAD:\n",
    "    # Sleep for 5 secs to avoid overwhelming the API with requests\n",
    "    time.sleep(5)\n",
    "    # Clear outputs to reduce clutter\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    execution = ai_api_v2_client.execution.get(execution_resp.id)\n",
    "    status = execution.status\n",
    "    print('...... execution status ......', flush=True)\n",
    "    print(f\"Training status: {execution.status}\")\n",
    "    print(f\"Training status details: {execution.status_details}\")\n",
    "\n",
    "\n",
    "if execution.status == Status.COMPLETED:\n",
    "    print(f\"Training complete for execution [{execution_resp.id}]!\")\n",
    "    output_artifact = execution.output_artifacts[0]\n",
    "    output = {\n",
    "        \"id\": output_artifact.id,\n",
    "        \"name\": output_artifact.name,\n",
    "        \"url\": output_artifact.url\n",
    "    }\n",
    "    with open('training_output.json', 'w') as fp:\n",
    "        json.dump(output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5210a9",
   "metadata": {},
   "source": [
    "# Metrics and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c16cc",
   "metadata": {},
   "source": [
    "A common need is to check the performance of the training. To that it is possible to define some metrics in the training code and register them inside AiCore. This will made them available once the training is complete.\n",
    "\n",
    "Below some example about how to retrieve the metrics from AICore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f1d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_string = \"executionId eq '\" + execution_resp.id + \"'\"\n",
    "metric_resp = ai_api_v2_client.metrics.query(execution_ids=execution_resp.id)\n",
    "\n",
    "for m in metric_resp.resources:\n",
    "    for metric in m.metrics:\n",
    "        print(metric.name)\n",
    "        print(metric.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe782d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "for m in metric_resp.resources:\n",
    "    for custom_info in m.custom_info:\n",
    "        #print(custom_info.name)\n",
    "        #print(custom_info.value)\n",
    "        all_metrics.append(custom_info.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = ast.literal_eval(all_metrics[0])\n",
    "confusion_matrix = ast.literal_eval(all_metrics[1])\n",
    "#print(training_metrics[0].get(\"loss\"))\n",
    "#print(confusion_matrix[0].get(\"actual label - 0\"))\n",
    "#print(confusion_matrix[1].get(\"actual label - 1\"))\n",
    "\n",
    "a = confusion_matrix[0].get(\"actual label - 0\")\n",
    "b = confusion_matrix[1].get(\"actual label - 1\")\n",
    "\n",
    "a = [int(x) for x in re.split('[^0-9]', a) if x]\n",
    "b = [int(x) for x in re.split('[^0-9]', b) if x]\n",
    "#print(a)\n",
    "#print(b)\n",
    "\n",
    "cnf_matrix = np.array([a, b])\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeeb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "a = ast.literal_eval(training_metrics[0].get(\"loss\"))\n",
    "b = ast.literal_eval(training_metrics[1].get(\"val_loss\"))\n",
    "c = ast.literal_eval(training_metrics[2].get(\"accuracy\"))\n",
    "d = ast.literal_eval(training_metrics[3].get(\"val_accuracy\"))\n",
    "\n",
    "axs[0].plot(a)\n",
    "axs[0].plot(b)\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].legend(['Train', 'Val'])\n",
    "\n",
    "axs[1].plot(c)\n",
    "axs[1].plot(d)\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ba0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"normaly\",\"anomaly\"], normalize=True,\n",
    "                      title='Confusion matrix with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a8ae8",
   "metadata": {},
   "source": [
    "# Deploy ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede87fb6",
   "metadata": {},
   "source": [
    "The model deployment is aimed at the creation and the deployment of a serving API (or artifact) that can be called to perform the inference step on some data.\n",
    "\n",
    "To do that, first one needs to provide a piece of code where it is specified the behaviour of the artifact and a proper environment where the serving artifact can run. Assuming the code has been developed, one has to create a proper Docker image for running the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b849b",
   "metadata": {},
   "source": [
    "In order to prepare the docker image with the proper content, a Docker file and a requirement.txt file have to be prepared. Then the image needs to be built and uploaded with the following commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49336ebc",
   "metadata": {},
   "source": [
    "- docker build -t YOUR_DOCKER_REPO/sound-serve:0.0.1 .\n",
    "- docker push docker.io/YOUR_DOCKER_REPO/sound-serve:0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_workflow_file = '../workflows_sound/serving_workflow_tutorial.yaml'\n",
    "with open(serving_workflow_file) as swf:\n",
    "    serving_workflow = yaml.safe_load(swf)\n",
    "\n",
    "scenario_id = serving_workflow['metadata']['labels']['scenarios.ai.sap.com/id']\n",
    "input_artifact_name = serving_workflow['spec']['inputs']['artifacts'][0]['name']\n",
    "executable_name = serving_workflow['metadata']['name']\n",
    "\n",
    "training_output = 'training_output.json'\n",
    "with open(training_output) as to:\n",
    "    serving_input = json.load(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_binding = {\n",
    "    \"key\": input_artifact_name,\n",
    "    \"artifact_id\": serving_input[\"id\"]\n",
    "}\n",
    "\n",
    "serve_configuration = {\n",
    "    \"name\": \"dev-tutorial-serving-configuration\",\n",
    "    \"scenario_id\": scenario_id,\n",
    "    \"executable_id\": executable_name,\n",
    "    \"parameter_bindings\": [],\n",
    "    \"input_artifact_bindings\": [ InputArtifactBinding(**artifact_binding) ]\n",
    "}\n",
    "\n",
    "serve_config_resp = ai_api_v2_client.configuration.create(**serve_configuration)\n",
    "\n",
    "assert serve_config_resp.message == 'Configuration created'\n",
    "\n",
    "pprint(vars(serve_config_resp))\n",
    "print(\"configuration for serving the model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_resp = ai_api_v2_client.deployment.create(serve_config_resp.id)\n",
    "pprint(vars(deployment_resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll deployment status\n",
    "status = None\n",
    "while status != Status.RUNNING and status != Status.DEAD:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "    deployment = ai_api_v2_client.deployment.get(deployment_resp.id)\n",
    "    status = deployment.status\n",
    "    print('...... deployment status ......', flush=True)\n",
    "    print(deployment.status)\n",
    "    pprint(deployment.status_details)\n",
    "\n",
    "    if deployment.status == Status.RUNNING:\n",
    "        print(f\"Deployment with {deployment_resp.id} complete!\")\n",
    "\n",
    "# Allow some time for deployment URL to get ready\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad911f",
   "metadata": {},
   "source": [
    "# Using deployed ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e6d50",
   "metadata": {},
   "source": [
    "Let's define the local path to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path_normal_sounds = glob.glob(\"../data/normal/*\")\n",
    "path_abnormal_sounds = glob.glob(\"../data/abnormal/*\")\n",
    "#print(path_normal_sounds)\n",
    "#print(path_abnormal_sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd14630",
   "metadata": {},
   "source": [
    "Let's visualize some audio clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d70d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def acoustic_feature_computation(clip,sr):\n",
    "    #scale, sr = librosa.load(clip)\n",
    "    scale=clip\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=scale, \n",
    "        sr=sr, \n",
    "        hop_length=512,\n",
    "        n_mels=64,\n",
    "        fmax=sr/2\n",
    "    )\n",
    "    log_mel = librosa.power_to_db(mel_spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    MFCCs=librosa.feature.mfcc(y=scale, sr=sr, n_mfcc=40, fmax=sr/2)\n",
    "    acoustic_features=np.concatenate((MFCCs,log_mel_spectrogram), axis=0)\n",
    "    return acoustic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_abn, sr_abn = librosa.load(path_abnormal_sounds[0])\n",
    "scale_norm, sr_norm = librosa.load(path_normal_sounds[0])\n",
    "#print(scale_abn, scale_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f20ee7",
   "metadata": {},
   "source": [
    "First let's visualize the two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18237b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_features_abn = acoustic_feature_computation(scale_abn,sr_abn)\n",
    "acoustic_features_norm = acoustic_feature_computation(scale_norm,sr_norm)\n",
    "print(acoustic_features_abn.shape)\n",
    "print(acoustic_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c006d9",
   "metadata": {},
   "source": [
    "Abnormal sound clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a7c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.pcolor(acoustic_features_abn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75b0e3",
   "metadata": {},
   "source": [
    "Normal sound clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba118ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(acoustic_features_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd7c32",
   "metadata": {},
   "source": [
    "In order to perform the inference step, let's transform one of the sound clips into a string (this will constitute  the body of the API call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664920f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "import base64\n",
    "import io\n",
    "from json import dumps\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "# first: reading the binary stuff\n",
    "# note the 'rb' flag\n",
    "# result: bytes\n",
    "with open(path_abnormal_sounds[0], 'rb') as open_file:\n",
    "    byte_content = open_file.read()\n",
    "\n",
    "# second: base64 encode read data\n",
    "# result: bytes (again)\n",
    "base64_bytes = b64encode(byte_content)\n",
    "\n",
    "# third: decode these bytes to text\n",
    "# result: string (in utf-8)\n",
    "base64_string = base64_bytes.decode(ENCODING)\n",
    "\n",
    "# optional: doing stuff with the data\n",
    "# result here: some dict\n",
    "raw_data = {\"sound\": base64_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"sounds.txt\", \"w\")\n",
    "f.write(base64_string)\n",
    "f.close()\n",
    "#print(base64_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ded1b9",
   "metadata": {},
   "source": [
    "Let's make a call to the API for retrieving the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42270",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{deployment.deployment_url}/v1/models/soundmodel:predict\"\n",
    "print(endpoint)\n",
    "\n",
    "headers = {\"Authorization\": ai_api_v2_client.rest_client.get_token(),\n",
    "           'ai-resource-group': resource_group,\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "response = requests.post(endpoint, headers=headers, json=raw_data)\n",
    "\n",
    "print('Inference result:', response.json())\n",
    "#pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f1de5",
   "metadata": {},
   "source": [
    "# Stop deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f48aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resp = ai_api_v2_client.deployment.modify(deployment_resp.id,\n",
    "                                                 target_status=TargetStatus.STOPPED)\n",
    "status = None\n",
    "while status != Status.STOPPED:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "    deployment = ai_api_v2_client.deployment.get(deployment_resp.id)\n",
    "    status = deployment.status\n",
    "    print('...... killing deployment ......', flush=True)\n",
    "    print(f\"Deployment status: {deployment.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3c0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
